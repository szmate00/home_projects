{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.8.0+cu111\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as tf\n",
    "from torch.autograd import Variable\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get feature maps from VGG\n",
    "def get_features(image, model, layers):\n",
    "    features = {}\n",
    "\n",
    "    x = image\n",
    "    for name, number in model._modules.items():\n",
    "        x = number(x)\n",
    "\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "            \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for image loading and transforming\n",
    "def load_img(path):\n",
    "  \n",
    "    img = Image.open(path).convert('RGB')\n",
    "    \n",
    "    transform = tf.Compose([\n",
    "                        tf.Resize(400),\n",
    "                        tf.ToTensor(),\n",
    "                        tf.Normalize((0.485, 0.456, 0.406), \n",
    "                                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "    img = transform(img)[:3,:,:].unsqueeze(0)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to transform result tensor back to image\n",
    "def tensor_to_img(tensor):\n",
    "    \n",
    "    img = tensor.to(\"cpu\").clone().detach()\n",
    "    img = img.numpy().squeeze()\n",
    "    img = img.transpose(1, 2, 0)\n",
    "    img = img * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    img = img.clip(0, 1)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for calculating gram matrix of feature map\n",
    "def gram_matrix(vec_in):\n",
    "    '''\n",
    "    Gram matrix should have shape of K * N, where K is the number of feature maps at the given layer,\n",
    "    N is the length of the vector after transforming the 2D feature map to 1D vector.\n",
    "    N = a * b if one feature map has a shape of a * b.\n",
    "    So input has a shape of K * a * b\n",
    "    '''\n",
    "    batch_size, K, a, b = vec_in.size()\n",
    "    vecs = vec_in.view(K, a * b)\n",
    "\n",
    "    # definition of Gram matrix\n",
    "    gram = vecs @ vecs.T\n",
    "\n",
    "    # returning normalized matrix\n",
    "    return gram / (K * a * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): ReLU(inplace=True)\n",
       "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (24): ReLU(inplace=True)\n",
       "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (26): ReLU(inplace=True)\n",
       "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (31): ReLU(inplace=True)\n",
       "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (33): ReLU(inplace=True)\n",
       "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (35): ReLU(inplace=True)\n",
       "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# loading pretrained VGG\n",
    "vgg = models.vgg19(pretrained=True).features\n",
    "\n",
    "# freezing weights\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad_(False)\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "vgg.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading images\n",
    "content = load_img('images/content.png').to(device)\n",
    "style = load_img('images/style.png').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using original content image to use style transfer on\n",
    "x = content.clone().requires_grad_(True).to(device)\n",
    "\n",
    "# using Adam to optimize our image\n",
    "optimizer = optim.Adam([x], lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers to caclulate stlye and content losses with (see 3rd cell)\n",
    "layers = {'0': 'conv1_1',\n",
    "            '5':  'conv2_1',\n",
    "            '10': 'conv3_1',\n",
    "            '19': 'conv4_1',\n",
    "            '21': 'conv4_2',\n",
    "            '28': 'conv5_1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target content feature map\n",
    "content_features = get_features(content, vgg, layers)\n",
    "\n",
    "# style content feature maps, we'll calculate gram matrices\n",
    "style_features = get_features(style, vgg, layers)\n",
    "\n",
    "style_layers = ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1']\n",
    "\n",
    "# calculating gram matrices of each style layer to use as targets\n",
    "style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_layers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "EPOCHS = 2000\n",
    "content_weight = 1e-3\n",
    "style_weight = 1e8\n",
    "\n",
    "style_weights = {layer: (1e3 / n**2) for layer, n in zip(style_layers, [64, 128, 256, 512, 512])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 100 | Total Loss: 38.13358\n",
      "Epoch: 200 | Total Loss: 15.50117\n",
      "Epoch: 300 | Total Loss: 8.42360\n",
      "Epoch: 400 | Total Loss: 5.33074\n",
      "Epoch: 500 | Total Loss: 3.74347\n",
      "Epoch: 600 | Total Loss: 2.85104\n",
      "Epoch: 700 | Total Loss: 2.29580\n",
      "Epoch: 800 | Total Loss: 1.91257\n",
      "Epoch: 900 | Total Loss: 1.62934\n",
      "Epoch: 1000 | Total Loss: 1.41282\n",
      "Epoch: 1100 | Total Loss: 1.24478\n",
      "Epoch: 1200 | Total Loss: 1.11299\n",
      "Epoch: 1300 | Total Loss: 1.00806\n",
      "Epoch: 1400 | Total Loss: 0.92197\n",
      "Epoch: 1500 | Total Loss: 0.84935\n",
      "Epoch: 1600 | Total Loss: 0.78677\n",
      "Epoch: 1700 | Total Loss: 0.73177\n",
      "Epoch: 1800 | Total Loss: 0.68278\n",
      "Epoch: 1900 | Total Loss: 0.63870\n",
      "Epoch: 2000 | Total Loss: 0.59861\n",
      "-----------------------------\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    # content loss\n",
    "    x_features = get_features(x, vgg, layers)\n",
    "    content_loss = F.mse_loss(x_features[\"conv4_2\"], content_features[\"conv4_2\"])\n",
    "\n",
    "    # style loss\n",
    "    # summing up the losses from each style layer\n",
    "    style_loss = 0\n",
    "    for layer in style_layers:\n",
    "        x_style = x_features[layer]\n",
    "        # each style layer has a separate weight too\n",
    "        style_loss += style_weights[layer] * F.mse_loss(gram_matrix(x_style), style_grams[layer])\n",
    "\n",
    "    # total loss is content loss + style loss with weights as seen in the paper\n",
    "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "            print('Epoch: %d | Total Loss: %.5f' % (epoch + 1, total_loss.item()))\n",
    "\n",
    "print('-----------------------------')\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming result tensor to numpy array\n",
    "transfer = tensor_to_img(x.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescaling the array for PIL\n",
    "rescaled = (255.0 / transfer.max() * (transfer - transfer.min())).astype(np.uint8)\n",
    "\n",
    "# saving the result image\n",
    "im = Image.fromarray(rescaled)\n",
    "im.save('images/transfer.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}